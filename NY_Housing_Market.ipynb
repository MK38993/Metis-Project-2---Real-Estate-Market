{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bsp\n",
    "from time import sleep, time\n",
    "from random import random\n",
    "from timeit import timeit\n",
    "import re\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (cross_val_score, train_test_split, \n",
    "                                     KFold, GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%run CHROMEDRIVER_DATA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c291478",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_link=\"https://www.realtor.com\"\n",
    "listing_page=\"https://www.realtor.com/soldhomeprices/New-York_NY/pg-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some important vars.\n",
    "all_pages=[]\n",
    "links=[]\n",
    "listing_pages=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURL(n):\n",
    "    return(basic_link+links[n])\n",
    "\n",
    "def sel(url, sleeptime=0, use_chrome=False):\n",
    "    browser = None\n",
    "    if use_chrome:\n",
    "        browser = webdriver.Chrome(chrome_path)\n",
    "    else:\n",
    "        browser = webdriver.Safari()\n",
    "    browser.get(url)\n",
    "    if sleeptime>0:\n",
    "        sleep(sleeptime)    \n",
    "    code = browser.page_source    \n",
    "    browser.close()\n",
    "    return(code)\n",
    "\n",
    "def req(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    code=requests.get(url, headers=headers)\n",
    "    return(code)\n",
    "\n",
    "def soupIt(code):\n",
    "    return (bsp(code, 'html.parser'))\n",
    "\n",
    "def getAllPages(pages):\n",
    "    webpages=[]\n",
    "    for page in range(1,pages+1):\n",
    "        url=listing_page+str(page)\n",
    "        webpages.append(sel(url))\n",
    "        \n",
    "def getAllListings(code):\n",
    "    \n",
    "    temp_listings=[]\n",
    "    \n",
    "def startTimer():\n",
    "    global start\n",
    "    start=time()\n",
    "    print('Timer started at system time',start)\n",
    "    \n",
    "def timeSince():\n",
    "    print('Time elapsed:',time()-start)\n",
    "\n",
    "def botDetected(code):\n",
    "    bot_text='As you were browsing, something about your browser made us think you might be a bot.'\n",
    "    if len(re.findall(bot_text,code))>0:\n",
    "        return(True)\n",
    "    return(False)\n",
    "\n",
    "#Used to fix null neighborhood/borough values.\n",
    "def geocode(address):\n",
    "    address+=',New York City, NY'\n",
    "    link='https://maps.googleapis.com/maps/api/geocode/json?address='\n",
    "    key='&key=<API Key here>'\n",
    "    address=address.replace(' ', '+')\n",
    "    address=address.replace('#', '')\n",
    "    r=requests.get(link+address+key)\n",
    "    r=r.text\n",
    "    r.replace('  ','')\n",
    "    #r.split('Denied'\n",
    "    \n",
    "    if len(raw)==1:\n",
    "        neighborhood=\"-3\"\n",
    "        borough=raw[0].split('long_name')[1].split('short_name')[0].replace(\"'\",'').replace(\":\",'').replace('\"','').replace(',','').strip()\n",
    "    else:\n",
    "        neighborhood=raw[0].split('long_name')[1].split('short_name')[0].replace(\"'\",'').replace(\":\",'').replace('\"','').replace(',','').strip()\n",
    "        borough=raw[1].split('long_name')[1].split('short_name')[0].replace(\"'\",'').replace(\":\",'').replace('\"','').replace(',','').strip()\n",
    "     \n",
    "    return(neighborhood, borough)\n",
    "\n",
    "def geocode_raw(address):\n",
    "    address+=',New York City, NY'\n",
    "    link='https://maps.googleapis.com/maps/api/geocode/json?address='\n",
    "    key='&key=<API Key here>'\n",
    "    address=address.replace(' ', '+')\n",
    "    address=address.replace('#', '')\n",
    "    r=requests.get(link+address+key)\n",
    "    r=r.text\n",
    "    r.replace('  ','')\n",
    "    return(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a9ed7",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea72051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [A LONG FREAKING TIME] seconds to run\n",
    "#Also your computer's out of commission for this time period thanks to the opening/closing of windows.\n",
    "#I wouldn't recommend running this if you already have all_listing_pages.txt .\n",
    "RESCRAPE_SEARCH_PAGES=False\n",
    "\n",
    "if RESCRAPE_SEARCH_PAGES:\n",
    "    startTimer()\n",
    "    all_pages=getAllPages(225)\n",
    "    timeSince()\n",
    "    conglomerate=\"\"\n",
    "    for html in all_pages:\n",
    "        conglomerate+=html\n",
    "        conglomerate+=\"_split_this_html_code_with_python\"\n",
    "    with open('all_listing_pages.txt','w') as f:\n",
    "        f.write(conglomerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4741609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [70] seconds to run\n",
    "RESET_PAGELIST=False\n",
    "\n",
    "if RESET_PAGELIST:\n",
    "    startTimer()\n",
    "    f=open('all_listing_pages.txt','r')\n",
    "    allcode=\"\"\n",
    "    for item in f:\n",
    "        allcode+=item\n",
    "    all_pages=allcode.split(\"_split_this_html_code_with_python\")\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.2] seconds to run\n",
    "RECREATE_LINK_LIST=False\n",
    "\n",
    "if RECREATE_LINK_LIST:\n",
    "    startTimer()\n",
    "    links=[]\n",
    "    for page in all_pages:\n",
    "        tempfindall=re.findall('(/realestateandhomes-detail/.+?)\" data-featured',page)\n",
    "        for link in tempfindall:\n",
    "            links.append(link)\n",
    "    with open('all_listing_links.txt','w') as f:\n",
    "        for link in links:\n",
    "            f.write(link+'\\n')        \n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.01] seconds to run\n",
    "RESET_LINK_LIST=True\n",
    "\n",
    "if RESET_LINK_LIST:\n",
    "    startTimer()\n",
    "    links=[]\n",
    "    f=open('all_listing_links.txt','r')\n",
    "    for link in f:\n",
    "        links.append(link.strip())\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [I HAVE DONE NOTHING BUT SCRAPE LISTINGS FOR THE LAST THREE DAYS] seconds to run\n",
    "#And I couldn't even walk away because Selenium kept running into bot-protection measures every 150 listings or so.\n",
    "#I already have the HTML...don't run this if you value your time/sanity :( \n",
    "\n",
    "RESCRAPE_LISTINGS=False\n",
    "\n",
    "if RESCRAPE_LISTINGS:\n",
    "    startTimer() \n",
    "    stopVar=9900\n",
    "    listing_code=listing_code[0:stopVar]\n",
    "    for i in range(stopVar,9900):\n",
    "        link_ext=links[i]\n",
    "        print(i)\n",
    "        listing_code.append(sel(basic_link+link_ext)) #,random()*1+2\n",
    "    for html in listing_code:\n",
    "        conglomerate+=html\n",
    "        conglomerate+=\"_split_this_html_code_with_python\"\n",
    "    with open('all_listings.txt','w') as f:\n",
    "        f.write(conglomerate)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c078410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [180] seconds to run\n",
    "#Turns out GitHub doesn't let you upload files that are 4GB in size.\n",
    "#Here's the code for extracting raw code from the HTML into a list.\n",
    "#Even though I never used this because I ran out of RAM\n",
    "\n",
    "RELOAD_RAW_CODE=False\n",
    "\n",
    "if RELOAD_RAW_CODE:\n",
    "    startTimer()\n",
    "    all_listings_raw=[]\n",
    "    with open('all_listings.txt','r') as f:\n",
    "        conglomerate=\"\"      \n",
    "        counter=0\n",
    "        for item in f:\n",
    "            conglomerate+=item\n",
    "            counter+=1\n",
    "            if counter%100000==0:\n",
    "                print(f'{counter/1000000}M complete')\n",
    "                slicer=conglomerate.split(\"_split_this_html_code_with_python\")\n",
    "                print(len(slicer))\n",
    "                all_listings_raw+=slicer[0:len(slicer)-1]\n",
    "                conglomerate=slicer[len(slicer)-1]\n",
    "                #print(conglomerate)\n",
    "        slicer=conglomerate.split(\"_split_this_html_code_with_python\")\n",
    "        all_listings_raw+=slicer\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2 of reloading raw code\n",
    "\n",
    "if RELOAD_RAW_CODE:\n",
    "    startTimer()\n",
    "    conglomerate=\"\"\n",
    "    counter=0\n",
    "    for item in all_listings_raw:\n",
    "        conglomerate+=item\n",
    "        counter+=1\n",
    "        if counter%10000==0:\n",
    "            all_listings\n",
    "    conglomerate.split(\"_split_this_html_code_with_python\")\n",
    "    timeSince()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut out 75% of unneeded code so I can actually run my code instead of running out of RAM\n",
    "SLICE_LISTING_CODE=False\n",
    "if SLICE_LISTING_CODE:\n",
    "    sliced=[]\n",
    "    counter=0\n",
    "    for listing in listing_code:\n",
    "        cut=listing.split('Property History')[1]\n",
    "        sliced.append(cut)\n",
    "        counter+=1\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [180] seconds to run\n",
    "#Reload listing code from file.\n",
    "\n",
    "RELOAD_SLICED_LISTINGS=False\n",
    "\n",
    "if RELOAD_SLICED_LISTINGS:\n",
    "    startTimer()\n",
    "    templist=[]\n",
    "    with open('all_listings_sliced.txt','r') as f:\n",
    "        for item in f:\n",
    "            templist.append(item)\n",
    "    conglomerate=\"\"   \n",
    "    templist2=[]\n",
    "    counter=len(templist)\n",
    "    for item in templist:\n",
    "        conglomerate+=item\n",
    "        counter-=1\n",
    "        if counter%500000==0:\n",
    "            templist2.append(conglomerate)\n",
    "            conglomerate=\"\"\n",
    "            print(f'{counter} lines left')\n",
    "    templist=[]\n",
    "    conglomerate=\"\"\n",
    "    counter=len(templist2)\n",
    "    for item in templist2:\n",
    "        conglomerate+=item\n",
    "        counter-=1\n",
    "        print(f'{counter} items left')\n",
    "    templist2=[]\n",
    "    listing_slice=conglomerate.split('_split_this_html_code_with_python')\n",
    "    conglomerate=\"\"    \n",
    "    listing_slice=listing_slice[0:9900]\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f998511",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1fc880",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Takes about [8] seconds to run\n",
    "#Create a dataframe from HTML code.\n",
    "RECREATE_LISTING_DATA=False\n",
    "\n",
    "if RECREATE_LISTING_DATA:  \n",
    "    startTimer()  \n",
    "    listing_data={}\n",
    "    listing_data['sqft']=[]#\n",
    "    listing_data['beds']=[]#\n",
    "    listing_data['baths']=[]#\n",
    "    listing_data['lot_size']=[]#\n",
    "    listing_data['year_built']=[]#\n",
    "    listing_data['address']=[]#\n",
    "    listing_data['property_type']=[]#\n",
    "    listing_data['neighborhood']=[]#\n",
    "    listing_data['borough']=[]#\n",
    "    #listing_data['stories']=[]\n",
    "    #listing_data['has_garage']=[]\n",
    "    #listing_data['has_deck']=[]\n",
    "    #listing_data['yard_type']=[]\n",
    "    listing_data['sale_price']=[]\n",
    "\n",
    "    for sliced in listing_slice:\n",
    "        #PROPERTY TYPE\n",
    "        code='''<i class=\"ra ra-property-type\"></i>\\n          <div>Type</div>\\n          <div class=\"key-fact-data ellipsis\" data-toggle=\"tooltip\" data-placement=\"top\" title=\"\" data-original-title=\"(.+?)\">(.+?)</div>'''\n",
    "        type_raw=re.findall(code,sliced)\n",
    "        if len(type_raw)!=0:\n",
    "            listing_data['property_type'].append(type_raw[0][0])\n",
    "        else:\n",
    "            listing_data['property_type'].append(\"-1\")\n",
    "\n",
    "        #YEAR BUILT\n",
    "        code='''<i class=\"ra ra-year-built\"></i>\\n          <div>Built</div>\\n          <div class=\"key-fact-data ellipsis\">([0-9]+)</div>'''\n",
    "        year_raw=re.findall(code,sliced)\n",
    "        if len(year_raw)!=0:\n",
    "            listing_data['year_built'].append(year_raw[0])\n",
    "        else:\n",
    "            listing_data['year_built'].append(-1)\n",
    "\n",
    "        #SQUARE FOOTAGE\n",
    "        code='''<li data-label=\"property-meta-sqft\">\\n      <span class=\"data-value\">([0-9,.]+)</span> sq ft'''\n",
    "        sqft_raw=re.findall(code,sliced)\n",
    "        if len(sqft_raw)>0:\n",
    "            sqft_raw=float(sqft_raw[0].replace(',',''))\n",
    "            listing_data['sqft'].append(sqft_raw)\n",
    "        else:\n",
    "            listing_data['sqft'].append(-1)\n",
    "\n",
    "        #BEDROOMS\n",
    "        code='''<li data-label=\"property-meta-beds\">\\n      <span class=\"data-value\">([0-9.,]+)</span>'''\n",
    "        beds_raw=re.findall(code,sliced)\n",
    "        if len(beds_raw)>0:\n",
    "            listing_data['beds'].append(float(beds_raw[0]))\n",
    "        else:\n",
    "            listing_data['beds'].append(-1)\n",
    "\n",
    "        #BATHROOMS    \n",
    "        code='''<li data-label=\"property-meta-bath\">\\n      <span class=\"data-value\">([0-9.,]+)</span>'''\n",
    "        baths_raw=re.findall(code,sliced)\n",
    "        if len(baths_raw)>0:\n",
    "            listing_data['baths'].append(float(baths_raw[0]))\n",
    "        else:\n",
    "            listing_data['baths'].append(-1)    \n",
    "\n",
    "        #LOT SIZE\n",
    "        code='''<li data-label=\"property-meta-lotsize\">\\n      <span class=\"data-value\">.+</span>\\n.+\\n    </li>'''\n",
    "        lotsize_raw=re.findall(code,sliced)\n",
    "        #print(lotsize_raw)\n",
    "        if len(lotsize_raw)>0:\n",
    "            code='<span class=\"data-value\">([0-9,.]+)</span>'\n",
    "            acre=False\n",
    "            acre=len(lotsize_raw[0].split('acre'))>1\n",
    "            acre_to_sqft=43560.04\n",
    "            lotsize_raw=re.findall(code, lotsize_raw[0])[0]\n",
    "            lotsize_raw=float(lotsize_raw.replace(',',''))\n",
    "            #Set a reasonable limit on lot size so listings accidentally mislabeled as \"acres\" won't ruin the data\n",
    "            if acre and lotsize_raw<5:\n",
    "                lotsize_raw*=acre_to_sqft\n",
    "            listing_data['lot_size'].append(lotsize_raw)\n",
    "        else:\n",
    "            listing_data['lot_size'].append(-1)    \n",
    "\n",
    "        #SALE PRICE\n",
    "        code=\"Last Sold for.+?[$]([0-9,.]+)\"\n",
    "        sale_raw=re.findall(code,sliced)\n",
    "        if len(sale_raw)>0:\n",
    "            listing_data['sale_price'].append(float(sale_raw[0].replace(',','')))\n",
    "        else:\n",
    "            listing_data['sale_price'].append(-1) \n",
    "\n",
    "        #ADDRESS\n",
    "        code='<span itemprop=\"streetAddress\">(.+?),</span>' \n",
    "        address_raw=re.findall(code,sliced)\n",
    "        if len(address_raw)>0:\n",
    "            listing_data['address'].append(address_raw[0])\n",
    "        else:\n",
    "            listing_data['address'].append(\"-1\")     \n",
    "\n",
    "        #NEIGHBORHOOD/BOROUGH\n",
    "        code=\"is located in <a href.+?>(.+?)</a> neighborhood in the city of <a href.+?>(.+?), NY.\" \n",
    "        area_raw=re.findall(code,sliced)\n",
    "        if len(area_raw)>0:\n",
    "            listing_data['neighborhood'].append(area_raw[0][0])\n",
    "            listing_data['borough'].append(area_raw[0][1])\n",
    "        else:\n",
    "            listing_data['neighborhood'].append(\"-1\")\n",
    "            listing_data['borough'].append(\"-1\")           \n",
    "\n",
    "        listing_frame=pd.DataFrame.from_dict(listing_data, orient='columns')\n",
    "\n",
    "        #Last \"listing\" is blank.\n",
    "        listing_frame.drop([9899], inplace=True)\n",
    "        \n",
    "        #Blank addresses can't be used.\n",
    "        listing_frame=listing_frame[listing_frame['address']==\"-1\"]\n",
    "        \n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.15] seconds to run\n",
    "#Save dataframe to CSV.\n",
    "\n",
    "SAVE_LISTING_FRAME=False\n",
    "if SAVE_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    listing_frame.to_csv('listing_frame1.csv')\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6110d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Takes about [0.07] seconds to run\n",
    "#Load dataframe from CSV.\n",
    "\n",
    "LOAD_LISTING_FRAME=False\n",
    "if LOAD_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    listing_frame=pd.read_csv('listing_frame1.csv')\n",
    "    listing_frame.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20284fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fill in missing neighborhood/borough values\n",
    "\n",
    "FILL_AREA_VALUES=False\n",
    "\n",
    "if FILL_AREA_VALUES:\n",
    "    startTimer()\n",
    "    for i in range(len(listing_frame['neighborhood'])):\n",
    "        if listing_frame.iloc[i]['neighborhood']==\"-1\" or listing_frame.iloc[i]['borough']==\"-1\":\n",
    "            print(i)\n",
    "            area=geocode(listing_frame.iloc[i]['address'])\n",
    "            listing_frame.at[i,'neighborhood']=area[0]\n",
    "            listing_frame.at[i,'borough']=area[1]\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.6] seconds to run\n",
    "#Remove street names with 'withheld' in them; these aren't real street names.\n",
    "PROCESS_LISTING_FRAME=False\n",
    "\n",
    "if PROCESS_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    for i in range(len(listing_frame['address'])):\n",
    "        if len(listing_frame.iloc[i]['address'].split('ithheld'))>1:\n",
    "            listing_frame.at[i,'address']=\"-4\"\n",
    "    listing_frame=listing_frame[listing_frame['address']!='-4']\n",
    "    listing_frame.reset_index(inplace=True)\n",
    "    listing_frame.drop(['index'], axis=1,inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ee13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make missing values np.NaN\n",
    "\n",
    "if PROCESS_LISTING_FRAME:\n",
    "    listing_frame.replace('-1', np.NaN, inplace=True)\n",
    "    listing_frame.replace(-1, np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ac096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.1] seconds to run\n",
    "#Save dataframe to CSV.\n",
    "\n",
    "SAVE_LISTING_FRAME=False\n",
    "if SAVE_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    listing_frame.to_csv('listing_frame2.csv')\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb986f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.05] seconds to run\n",
    "#Load dataframe from CSV.\n",
    "\n",
    "LOAD_LISTING_FRAME=False\n",
    "if LOAD_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    listing_frame=pd.read_csv('listing_frame2.csv')\n",
    "    listing_frame.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with no null values\n",
    "\n",
    "CREATE_NULLLESS=False\n",
    "if CREATE_NULLLESS:\n",
    "    \n",
    "    nullless=listing_frame.copy()\n",
    "    nullless.drop(['neighborhood'], axis=1, inplace=True)\n",
    "    nullless.dropna(axis=0,how='any', inplace=True)\n",
    "    nullless.reset_index(inplace=True)\n",
    "    nullless.drop(['index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ac962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Takes about [0.03] seconds to run\n",
    "#Save nullless dataframe to CSV.\n",
    "\n",
    "SAVE_LISTING_FRAME=False\n",
    "if SAVE_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    nullless.to_csv('nullless.csv')\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6240c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.04] seconds to run\n",
    "#Load nullless dataframe from CSV.\n",
    "\n",
    "LOAD_LISTING_FRAME=False\n",
    "if LOAD_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    nullless=pd.read_csv('nullless.csv')\n",
    "    nullless.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a8c6f",
   "metadata": {},
   "source": [
    "# Preparing Data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4603631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dummy variables.\n",
    "DUMMY_OUT=False\n",
    "if DUMMY_OUT:\n",
    "    dum_bor=pd.get_dummies(nullless['borough'])\n",
    "    #dum_nei=pd.get_dummies(nullless['neighborhood'])\n",
    "    dum_ptp=pd.get_dummies(nullless['property_type'])\n",
    "\n",
    "    #Replace categorical variables with dummies.\n",
    "    nullless.drop(['borough','neighborhood','property_type'], inplace=True, axis=1)\n",
    "    nullless=nullless.join(dum_bor)\n",
    "    #nullless=nullless.join(dum_nei)\n",
    "    nullless=nullless.join(dum_ptp)\n",
    "\n",
    "    #Move sale_price to the end of the dataframe.\n",
    "    nullless[\"sale_price2\"]=nullless[\"sale_price\"]\n",
    "    nullless.drop(['sale_price'],axis=1, inplace=True)\n",
    "    nullless.rename(columns={'sale_price2':'sale_price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b11863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.03] seconds to run\n",
    "#Save nullless-dummied dataframe to CSV.\n",
    "\n",
    "SAVE_LISTING_FRAME=False\n",
    "if SAVE_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    nullless.to_csv('nulllessd.csv')\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.04] seconds to run\n",
    "#Load nullless-dummied dataframe from CSV.\n",
    "\n",
    "LOAD_LISTING_FRAME=False\n",
    "if LOAD_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    nullless=pd.read_csv('nulllessd.csv')\n",
    "    nullless.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize all non-sale-price values\n",
    "CREATE_STANDARD=False\n",
    "if CREATE_STANDARD:\n",
    "    scaler = StandardScaler()\n",
    "    nullless[['sqft', 'beds', 'baths', 'lot_size', 'year_built']] = scaler.fit_transform(nullless[['sqft', 'beds', 'baths', 'lot_size', 'year_built']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in missing neighborhood/borough values\n",
    "FILL_MISSING=False\n",
    "\n",
    "if FILL_MISSING:\n",
    "    startTimer()\n",
    "    for i in range(len(nullless['New York'])):\n",
    "        if nullless.iloc[i]['New York']==1:\n",
    "            b=geocode_raw(nullless.iloc[i]['address'])\n",
    "            for bor_name in ['Bronx','Brooklyn','Manhattan','Queens','Staten Island']:\n",
    "                if len(b.split(bor_name))>1:\n",
    "                    nullless.at[i,bor_name]=1\n",
    "    nullless.drop(['New York'],axis=1,inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c97c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.03] seconds to run\n",
    "#Save nullless-dummied-standardized dataframe to CSV.\n",
    "\n",
    "SAVE_LISTING_FRAME=False\n",
    "if SAVE_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    nullless.to_csv('nulllessds.csv')\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about [0.04] seconds to run\n",
    "#Load nullless-dummied-standardized dataframe from CSV.\n",
    "\n",
    "LOAD_LISTING_FRAME=True\n",
    "if LOAD_LISTING_FRAME:\n",
    "    startTimer()\n",
    "    nullless=pd.read_csv('nulllessds.csv')\n",
    "    nullless.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    timeSince()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88122ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nullless)):\n",
    "    n=0\n",
    "    for bor_name in ['Bronx','Brooklyn','Manhattan','Queens','Staten Island']:\n",
    "        if nullless.loc[i][bor_name]==1:\n",
    "            n+=1\n",
    "    if n!=1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b922008",
   "metadata": {},
   "source": [
    "# Compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nullless.drop(['sale_price', 'address'], axis=1)\n",
    "y = nullless['sale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83875b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxes={}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "#Run Linear Regression as a baseline\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "r2train=reg.score(X_train, y_train)\n",
    "r2test=reg.score(X_test, y_test)\n",
    "\n",
    "maxes['LinearRegression']=(r2test)\n",
    "\n",
    "\n",
    "\n",
    "#Run Lasso Regression and find best alpha value.\n",
    "max_i=0\n",
    "max_r2=0\n",
    "\n",
    "for i in np.arange(0,1,0.01):\n",
    "\n",
    "    reg = Lasso(alpha=i, max_iter=10000)\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    r2train=reg.score(X_train, y_train)\n",
    "    r2test=reg.score(X_test, y_test)\n",
    "    if r2test>max_r2:\n",
    "        max_r2=r2test\n",
    "        max_i=i\n",
    "maxes['Lasso']=(max_r2,max_i)\n",
    "\n",
    "#Run Ridge Regression and find best alpha value.\n",
    "max_i=0\n",
    "max_r2=0\n",
    "\n",
    "for i in np.arange(0,1,0.01):\n",
    "\n",
    "    reg = Ridge(alpha=i, max_iter=10000)\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    r2train=reg.score(X_train, y_train)\n",
    "    r2test=reg.score(X_test, y_test)\n",
    "    if r2test>max_r2:\n",
    "        max_r2=r2test\n",
    "        max_i=i\n",
    "maxes['Ridge']=(max_r2,max_i)\n",
    "\n",
    "#Run Elastic Net Regression and find best alpha value.\n",
    "max_i=0\n",
    "max_r2=0\n",
    "\n",
    "for i in np.arange(0,1,0.01):\n",
    "\n",
    "    reg = ElasticNet(alpha=i, max_iter=10000)\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    r2train=reg.score(X_train, y_train)\n",
    "    r2test=reg.score(X_test, y_test)\n",
    "    if r2test>max_r2:\n",
    "        max_r2=r2test\n",
    "        max_i=i\n",
    "maxes['ElasticNet']=(max_r2,max_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c49262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression gives best results thanks to floating point errors. I'm just using Linear Regression.\n",
    "maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ccde5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run Ridge model.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2train=model.score(X_train, y_train)\n",
    "r2test=model.score(X, y)\n",
    "if r2test>max_r2:\n",
    "    max_r2=r2test\n",
    "    max_i=i\n",
    "print('Regression: R^2 score on training set', r2train)\n",
    "print('Regression: R^2 score on test set', r2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals=[]\n",
    "pred=model.predict(X)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y_r=y.iloc[i]\n",
    "    y_p=pred[i]\n",
    "    r=y_r-y_p\n",
    "    residuals.append(r)\n",
    "\n",
    "nullless['predictions']=pred\n",
    "nullless['residuals']=residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac9bd7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot predicted values vs. observed values\n",
    "PRED_VS_OBS=True\n",
    "\n",
    "if PRED_VS_OBS:\n",
    "    plt.figure(figsize=(15,15)) \n",
    "    plt.xlabel('Observed Sale Price (thousands)')\n",
    "    plt.ylabel('Predicted Sale Price (thousands)')\n",
    "    plt.title(\"Observed Sale Price vs. Predicted Sale Price\")\n",
    "\n",
    "    plt.ylim(0,2000)\n",
    "    plt.xlim(0,2000)\n",
    "\n",
    "    plt.scatter(nullless['sale_price']/1000,nullless['predictions']/1000, alpha=0.2, s=100)\n",
    "    plt.plot(np.arange(0,2000),np.arange(0,2000),c='#e40000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ae3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "total=0\n",
    "for item in nullless['sale_price']:\n",
    "    total+=item\n",
    "total/=len(nullless['sale_price'])\n",
    "\n",
    "RMSE=mean_squared_error(nullless['sale_price'], nullless['predictions'], squared=False)\n",
    "print(total, RMSE, RMSE/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70214d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#253525.11415478506 0.3197742172222084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1ae96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nullless=pd.read_csv('nullless.csv')\n",
    "nullless.drop(['Unnamed: 0'],axis=1,inplace=True )\n",
    "\n",
    "CONTINUOUS_HEATMAP=False\n",
    "\n",
    "if CONTINUOUS_HEATMAP:\n",
    "    corr = nullless.corr()\n",
    "\n",
    "    sns.set(rc={\"figure.figsize\":(8, 8)})\n",
    "\n",
    "    g = sns.heatmap(corr,cmap='seismic', annot=True,\n",
    "            yticklabels=corr.columns)\n",
    "\n",
    "    g.set_xticklabels(corr.columns,rotation=0)\n",
    "    g.set_yticklabels(corr.columns,rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75749bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing_dict(sq,bed,bath,lot,year,ptype,borough):\n",
    "    d={}\n",
    "    d['sqft']=[sq]\n",
    "    d['beds']=[bed]\n",
    "    d['baths']=[bath]\n",
    "    d['lot_size']=[lot]\n",
    "    d['year_built']=[year]\n",
    "    \n",
    "    d['Bronx']=[0]\n",
    "    d['Brooklyn']=[0]\n",
    "    d['Manhattan']=[0]\n",
    "    d['Queens']=[0]\n",
    "    d['Staten Island']=[0]\n",
    "    \n",
    "    d['Condo']=[0]\n",
    "    d['Land']=[0]\n",
    "    d['Multi-Family Home']=[0]\n",
    "    d['Single Family Home']=[0]\n",
    "    \n",
    "    d[ptype]=[1]\n",
    "    d[borough]=[1]\n",
    "\n",
    "    df=pd.DataFrame.from_dict(d)\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def predict_house_price(sq,bed,bath,lot,year,ptype,borough):\n",
    "    inp=listing_dict(sq,bed,bath,lot,year,ptype,borough)\n",
    "\n",
    "    print(f'Input: {sq} sqft, {bed} bedrooms, {bath} bathrooms, {lot} lot size, built in {year}, property type {ptype}, located in {borough}, NYC')\n",
    "    standard=pd.read_csv('nulllessd.csv')\n",
    "    standard.drop(['Unnamed: 0', 'New York', 'address'], axis=1, inplace=True)\n",
    "    standard=standard.append(inp.loc[0])\n",
    "    #Standardize all non-sale-price values\n",
    "    scaler = StandardScaler()\n",
    "    standard[['sqft', 'beds', 'baths', 'lot_size', 'year_built']] = scaler.fit_transform(standard[['sqft', 'beds', 'baths', 'lot_size', 'year_built']])\n",
    "    \n",
    "    prediction_model = standard.drop(['sale_price'], axis=1)\n",
    "    out=model.predict(prediction_model)\n",
    "    print('Predicted price:')\n",
    "    return(out[len(out)-1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b5e3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#predict_house_price(1120,4,1,940,1290,'Single Family Home','Manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.coef_\n",
    "\n",
    "# Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulllessds=pd.read_csv('nulllessd.csv')\n",
    "# nulllessds.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "# nulllessds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nullless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard=pd.read_csv('nulllessd.csv')\n",
    "standard.drop(['Unnamed: 0', 'New York', 'address'], axis=1, inplace=True)\n",
    "standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced35ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
